# Ralph Progress Log
Loop: network-intelligence
Started: 2026-02-18
---

## Codebase Patterns
- Python venv at `.venv/` — activate with `source .venv/bin/activate`
- Supabase pagination: `.range(offset, offset + page_size - 1)` for >1000 rows
- Concurrent batch processing: `ThreadPoolExecutor(max_workers=8)` with batches
- API keys in `.env` loaded via `python-dotenv`
- JSONB enrichment fields may be string-wrapped — use `json.loads()` if string
- Supabase MCP tools available for SQL execution
- Existing enrichment script: `scripts/enrichment/enrich_contacts_apify.py`

---

## Iteration Log

### Iteration 1 — US-001: Add Intelligence Columns and Indexes
**Date:** 2026-02-18
**Status:** Complete

**What was done:**
- Applied Supabase migration via `mcp__supabase__apply_migration` (named `add_intelligence_columns_and_indexes`)
- Added 14 new columns to the `contacts` table:
  - Layer 1 (LLM tags): `ai_tags` (jsonb), `ai_tags_generated_at` (timestamptz), `ai_tags_model` (text)
  - Layer 2 (embeddings): `profile_embedding` (vector(768)), `interests_embedding` (vector(768))
  - Layer 3 (comms): `communication_history` (jsonb), `comms_last_gathered_at` (timestamptz)
  - Denormalized scores: `ai_proximity_score` (int), `ai_proximity_tier` (text), `ai_capacity_score` (int), `ai_capacity_tier` (text), `ai_kindora_prospect_score` (int), `ai_kindora_prospect_type` (text), `ai_outdoorithm_fit` (text)
- Created 4 indexes:
  - `idx_contacts_profile_embedding` — HNSW on profile_embedding (vector_cosine_ops, m=16, ef_construction=64)
  - `idx_contacts_interests_embedding` — HNSW on interests_embedding (vector_cosine_ops, m=16, ef_construction=64)
  - `idx_contacts_proximity_capacity` — Composite btree on (ai_proximity_tier, ai_capacity_tier) WHERE ai_proximity_score IS NOT NULL
  - `idx_contacts_ai_tags` — GIN on ai_tags with jsonb_path_ops
- Verified all columns via `information_schema.columns` query
- Verified all indexes via `pg_indexes` query

**Learnings:**
- Used `apply_migration` (not `execute_sql`) for DDL — this is the recommended approach per Supabase MCP docs
- All `ALTER TABLE ADD COLUMN IF NOT EXISTS` for idempotency
- All `CREATE INDEX IF NOT EXISTS` for idempotency
- Migration name used snake_case as required

**Files changed:** None (migration was applied directly to Supabase, no local script needed)

---

### Iteration 2 — US-002: Write LLM Tagging Script with Pydantic Schema
**Date:** 2026-02-18
**Status:** Complete

**What was done:**
- Created `scripts/intelligence/` directory
- Created `scripts/intelligence/tag_contacts_gpt5m.py` — full batch tagging script (310 lines)
- Pydantic models:
  - `ContactIntelligence` (top-level): relationship_proximity, giving_capacity, topical_affinity, sales_fit, outreach_context
  - Enums for tiers: ProximityTier, CapacityTier, ProspectType, FitLevel, TopicStrength, BestApproach
  - Nested models: SharedEmployer, SharedSchool, TopicTag, etc.
- Anchor profile: Justin's full career (Kindora, True Steele, Outdoorithm, Google.org, Year Up, Bridgespan, Bain + HBS, HKS, UVA + SF Foundation, Outdoorithm Collective boards)
- System prompt with detailed scoring guidelines per Section 5 & 8 of planning doc
- Context assembly: parses all JSONB enrichment fields (employment, education, skills, volunteering, certifications, publications, honors/awards), handles string-wrapped and native JSONB
- OpenAI integration: Uses `responses.parse()` with Pydantic `text_format` for guaranteed schema compliance
- Batch processing: `ThreadPoolExecutor(max_workers=10)` with Supabase pagination
- Saves to DB: full `ai_tags` JSONB + 7 denormalized score columns + metadata (generated_at, model)
- Flags: `--test` (5 contacts), `--dry-run` (no API calls), `--force` (re-tag), `--workers N`
- Error handling: 3 retries with exponential backoff for rate limits, graceful skip on failure
- Progress logging: contact-level output, periodic batch progress, final summary with token/cost tracking

**Dry-run verification:**
- `--dry-run --test`: 5 contacts, ~12K input tokens estimated
- `--dry-run` (full): 2,498 contacts, ~5.8M input tokens, ~2M output tokens, est. cost ~$2.08
- All imports clean, Pydantic models validate correctly

**Learnings:**
- OpenAI SDK v2.21 supports `client.responses.parse()` with `text_format=PydanticModel` — clean structured output
- `gpt-5-mini` is available (model ID: `gpt-5-mini-2025-08-07`)
- JSONB enrichment fields are stored as stringified JSON strings in Supabase — need `json.loads()` before passing to prompt
- Empty certifications exist (all null fields) — filter them out before including in prompt
- Average input tokens per contact: ~2,340 (range: 371-7,750 depending on enrichment depth)

**Files changed:**
- `scripts/intelligence/tag_contacts_gpt5m.py` (new)

---
