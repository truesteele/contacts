# Ralph Progress Log
Loop: network-intelligence
Started: 2026-02-18
---

## Codebase Patterns
- Python venv at `.venv/` — activate with `source .venv/bin/activate`
- Supabase pagination: `.range(offset, offset + page_size - 1)` for >1000 rows
- Concurrent batch processing: `ThreadPoolExecutor(max_workers=8)` with batches
- API keys in `.env` loaded via `python-dotenv`
- JSONB enrichment fields may be string-wrapped — use `json.loads()` if string
- Supabase MCP tools available for SQL execution
- Existing enrichment script: `scripts/enrichment/enrich_contacts_apify.py`

---

## Iteration Log

### Iteration 1 — US-001: Add Intelligence Columns and Indexes
**Date:** 2026-02-18
**Status:** Complete

**What was done:**
- Applied Supabase migration via `mcp__supabase__apply_migration` (named `add_intelligence_columns_and_indexes`)
- Added 14 new columns to the `contacts` table:
  - Layer 1 (LLM tags): `ai_tags` (jsonb), `ai_tags_generated_at` (timestamptz), `ai_tags_model` (text)
  - Layer 2 (embeddings): `profile_embedding` (vector(768)), `interests_embedding` (vector(768))
  - Layer 3 (comms): `communication_history` (jsonb), `comms_last_gathered_at` (timestamptz)
  - Denormalized scores: `ai_proximity_score` (int), `ai_proximity_tier` (text), `ai_capacity_score` (int), `ai_capacity_tier` (text), `ai_kindora_prospect_score` (int), `ai_kindora_prospect_type` (text), `ai_outdoorithm_fit` (text)
- Created 4 indexes:
  - `idx_contacts_profile_embedding` — HNSW on profile_embedding (vector_cosine_ops, m=16, ef_construction=64)
  - `idx_contacts_interests_embedding` — HNSW on interests_embedding (vector_cosine_ops, m=16, ef_construction=64)
  - `idx_contacts_proximity_capacity` — Composite btree on (ai_proximity_tier, ai_capacity_tier) WHERE ai_proximity_score IS NOT NULL
  - `idx_contacts_ai_tags` — GIN on ai_tags with jsonb_path_ops
- Verified all columns via `information_schema.columns` query
- Verified all indexes via `pg_indexes` query

**Learnings:**
- Used `apply_migration` (not `execute_sql`) for DDL — this is the recommended approach per Supabase MCP docs
- All `ALTER TABLE ADD COLUMN IF NOT EXISTS` for idempotency
- All `CREATE INDEX IF NOT EXISTS` for idempotency
- Migration name used snake_case as required

**Files changed:** None (migration was applied directly to Supabase, no local script needed)

---

### Iteration 2 — US-002: Write LLM Tagging Script with Pydantic Schema
**Date:** 2026-02-18
**Status:** Complete

**What was done:**
- Created `scripts/intelligence/` directory
- Created `scripts/intelligence/tag_contacts_gpt5m.py` — full batch tagging script (310 lines)
- Pydantic models:
  - `ContactIntelligence` (top-level): relationship_proximity, giving_capacity, topical_affinity, sales_fit, outreach_context
  - Enums for tiers: ProximityTier, CapacityTier, ProspectType, FitLevel, TopicStrength, BestApproach
  - Nested models: SharedEmployer, SharedSchool, TopicTag, etc.
- Anchor profile: Justin's full career (Kindora, True Steele, Outdoorithm, Google.org, Year Up, Bridgespan, Bain + HBS, HKS, UVA + SF Foundation, Outdoorithm Collective boards)
- System prompt with detailed scoring guidelines per Section 5 & 8 of planning doc
- Context assembly: parses all JSONB enrichment fields (employment, education, skills, volunteering, certifications, publications, honors/awards), handles string-wrapped and native JSONB
- OpenAI integration: Uses `responses.parse()` with Pydantic `text_format` for guaranteed schema compliance
- Batch processing: `ThreadPoolExecutor(max_workers=10)` with Supabase pagination
- Saves to DB: full `ai_tags` JSONB + 7 denormalized score columns + metadata (generated_at, model)
- Flags: `--test` (10 contacts default, configurable with `--count N`), `--dry-run` (no API calls), `--force` (re-tag), `--workers N`
- Error handling: 3 retries with exponential backoff for rate limits, graceful skip on failure
- Progress logging: contact-level output, periodic batch progress, final summary with token/cost tracking

**Dry-run verification:**
- `--dry-run --test`: 5 contacts, ~12K input tokens estimated
- `--dry-run` (full): 2,498 contacts, ~5.8M input tokens, ~2M output tokens, est. cost ~$2.08
- All imports clean, Pydantic models validate correctly

**Learnings:**
- OpenAI SDK v2.21 supports `client.responses.parse()` with `text_format=PydanticModel` — clean structured output
- `gpt-5-mini` is available (model ID: `gpt-5-mini-2025-08-07`)
- JSONB enrichment fields are stored as stringified JSON strings in Supabase — need `json.loads()` before passing to prompt
- Empty certifications exist (all null fields) — filter them out before including in prompt
- Average input tokens per contact: ~2,340 (range: 371-7,750 depending on enrichment depth)

**Files changed:**
- `scripts/intelligence/tag_contacts_gpt5m.py` (new)

---

### Iteration 3 — US-003: Test LLM Tagging on 10 Contacts and Validate Output Quality
**Date:** 2026-02-18
**Status:** Complete

**What was done:**
- Updated `--test` flag to default to 10 contacts (was 5), added `--count N` / `-n` parameter
- Ran `python scripts/intelligence/tag_contacts_gpt5m.py --test --count 10` — 10/10 contacts tagged, 0 errors
- **Critical bug fix:** `save_tags()` was using `json.dumps(tags_json)` which double-encoded the JSONB (string inside JSONB). Fixed by passing the dict directly. This caused `->` operator queries to fail on `ai_tags`.
- After fix, re-ran the test: all 10 contacts re-tagged with proper native JSONB storage
- Verified via SQL that `ai_tags->'topical_affinity'->'topics'` returns proper arrays (not null)

**Test contacts processed (10):**
| ID | Name | Proximity | Capacity | Kindora |
|----|------|-----------|----------|---------|
| 1 | Jeanne Byrd Adams | 65 (close) | 55 (mid_level) | 58 |
| 2 | Eugene Kirpichov | 30 (familiar) | 50 (mid_level) | 75 |
| 3 | Dakarai Aarons | 45 (warm) | 60 (mid_level) | 75 |
| 7 | Dreama Gentry | 48 (warm) | 55 (mid_level) | 82 |
| 9 | Miguel Santana | 55 (warm) | 95 (major_donor) | 90 |
| 10 | Kay Fernandez Smith | 78 (close) | 72 (major_donor) | 82 |
| 11 | Steffani Maxwell | 34 (familiar) | 48 (mid_level) | 72 |
| 12 | Precious Freeman, CFRE | 15 (acquaintance) | 45 (mid_level) | 65 |
| 13 | Casey Recupero | 68 (close) | 55 (mid_level) | 78 |
| 14 | Trina Villanueva | 50 (warm) | 62 (mid_level) | 72 |

**Spot-check results (all pass):**
- Shared employer → proximity >= 60: Kay (SF Foundation, 78), Casey (Year Up, 68), Jeanne (Year Up, 65) ✓
- C-suite → major_donor or mid_level: Miguel (CEO of CCF, 95 major_donor), Dreama (CEO, 55 mid_level) ✓
- Topics non-empty: All 10 contacts have 5-9 relevant topic tags with evidence ✓
- Personalization hooks: All 10 contacts have 3-5 specific, actionable hooks ✓
- Score distribution: 3 close, 4 warm, 2 familiar, 1 acquaintance — good spread, not 94% distant like old Perplexity scoring ✓

**Cost of 10-contact test:**
- Input tokens: 35,929
- Output tokens: 26,868
- Cost: $0.02
- Time: 448.9s (~44.9s/contact sequential)
- Extrapolated full batch: ~$5.00 for 2,498 contacts (higher than dry-run estimate due to actual output tokens being larger than 800 estimate)

**Quality observations:**
- Model sometimes varies proximity scores between runs for borderline cases (Eugene Kirpichov: 68 in run 1 vs 30 in run 2). Without confirmed temporal overlap at shared employers, the model is uncertain. Layer 3 (comms history) will help resolve these.
- Kindora prospect scores skew high (55-90 range) — may need calibration if too many contacts get flagged as prospects. Acceptable for now.
- Output quality is excellent: reasoning is detailed, personalization hooks reference specific shared experiences, topic tags have evidence strings.

**Learnings:**
- **CRITICAL:** Do NOT `json.dumps()` before passing to Supabase update for JSONB columns — pass the dict directly. Double-encoding breaks `->` operator queries.
- gpt-5-mini model works reliably for structured output — 100% success rate on 10 contacts
- Sequential test mode takes ~45s/contact; concurrent full batch should be much faster
- Output tokens per contact average ~2,687 (higher than the 800 estimate in planning doc) — this increases cost

**Files changed:**
- `scripts/intelligence/tag_contacts_gpt5m.py` (modified: added --count param, fixed JSONB double-encoding bug)

---
