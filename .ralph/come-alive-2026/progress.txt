# Ralph Progress Log
Loop: come-alive-2026
Started: 2026-02-23
---

## Codebase Patterns
- score_ask_readiness.py is the template for scaffold_campaign.py
- Pydantic schemas with openai.responses.parse() for structured output
- ThreadPoolExecutor(max_workers=150) for GPT-5 mini batch processing
- Supabase pagination: .range(offset, offset + page_size - 1)
- _strip_null_bytes() for PostgreSQL JSONB compatibility
- Env vars: OPENAI_APIKEY, ANTHROPIC_API_KEY, SUPABASE_URL, SUPABASE_KEY
- Strategy docs in docs/Outdoorithm/: DONOR_SEGMENTATION.md, COME_ALIVE_2026_Campaign.md, OC_FUNDRAISING_PLAYBOOK.md
- Python venv at .venv/, activate with source .venv/bin/activate
- GPT-5 mini does NOT support temperature=0
- supabase-contacts MCP server for DB operations

---

## Iteration Log

### Iteration 1 — US-001: Setup (2026-02-23)
**Status:** Complete
**What was done:**
- Added `campaign_2026` JSONB column to contacts table via `apply_migration` MCP tool
- Verified column exists: `{column_name: campaign_2026, data_type: jsonb}`
- Installed anthropic SDK v0.83.0 in .venv
- Verified both anthropic (0.83.0) and openai (2.21.0) imports work
**Learnings:**
- Use `apply_migration` (not `execute_sql`) for DDL operations per Supabase MCP guidance
- anthropic 0.83.0 installed cleanly with no dependency conflicts

### Iteration 2 — US-002: Create scaffold_campaign.py (2026-02-23)
**Status:** Complete
**What was done:**
- Created `scripts/intelligence/scaffold_campaign.py` — full campaign scaffolding script
- Pydantic schema `CampaignScaffold` with all 15 fields: persona, persona_confidence, persona_reasoning, campaign_list, capacity_tier, primary_ask_amount, motivation_flags, primary_motivation, lifecycle_stage, lead_story, story_reasoning, opener_insert, personalization_sentence, thank_you_variant, text_followup
- Comprehensive system prompt (~4,500 tokens) embedding:
  - Full persona decision tree (Believer → Impact Professional → Network Peer)
  - Execution matrix: Persona × Lifecycle → opener inserts
  - Ask anchor table: Persona × Capacity → dollar amounts
  - Motivation flags with detection signals (6 flags)
  - Story bank with matching rules (9 stories including "skip")
  - Thank-you frame: Persona × Motivation → thank-you variants
  - Follow-up timing matrix
  - Justin's voice guide and bio context
- Contact context builder reuses patterns from score_ask_readiness.py: ask_readiness, oc_engagement, communication_history, employment, education, shared_institutions, FEC, real estate, ai_tags, LinkedIn reactions
- Contact selection: ready_now (addressable) + cultivate_first score>=60 (addressable) + Tier 1 inner circle names
- Campaign list assignment logic: Tier 1 names → A, ready_now → B, cultivate_first >=76 → C, 60-75 → D
- JSONB merge preserves existing keys (personal_outreach, campaign_copy)
- `_strip_null_bytes()` for PostgreSQL JSONB compatibility
- CLI args: --test, --batch, --workers, --force, --contact-id
- ThreadPoolExecutor with 150 default workers, retry logic for rate limits and DB errors
- Test run: 1 contact scaffolded successfully, data saved to Supabase correctly
**Learnings:**
- The Supabase env var for the service key is `SUPABASE_SERVICE_KEY` (used in score_ask_readiness.py), not `SUPABASE_KEY` as documented in CLAUDE.md
- GPT-5 mini takes ~45s for a single scaffold call (heavier prompt than ask-readiness due to embedded strategy content)
- The `openai.responses.parse()` with `text_format=CampaignScaffold` works perfectly with all enum types
- Contact context is ~6K tokens per contact (ask_readiness summary + OC engagement + comms + employment + real estate + FEC)
- System prompt is the critical piece — embedding the full execution matrices and story bank ensures GPT makes correct assignments

### Iteration 3 — US-003: Run scaffold_campaign.py and Verify Output (2026-02-23)
**Status:** Complete
**What was done:**
- Ran `scaffold_campaign.py` on full campaign universe — 317 contacts scaffolded (316 new + 1 from test)
- 0 errors, 82.8s runtime, $0.65 cost (1.9M input tokens, 613K output tokens)
- Distribution: Believer 139, Impact Professional 151, Network Peer 27
- Campaign lists: A=25, B=107, C=42, D=143
- Capacity: Leadership 41, Major 137, Mid 113, Base 24, Community 2
- Lifecycle: New 289, Prior Donor 20, Lapsed 8
- Queried Supabase to verify distributions match script output (they do, +1 from prior test)
- Spot-checked 5 contacts across all list types and personas:
  1. Jose Gordon (A, Believer, major/$5K, skip story) — correct inner circle assignment
  2. Jesús García-Valadez (B, Impact Pro, major/$5K, carl story) — correct equity professional
  3. Roger Dean Huffstetler (C, Network Peer, major/$5K, dorian story) — correct HBS/HKS peer
  4. Nicki Anselmo (D, Prior Donor, Impact Pro, base/$1K, carl) — correct Google.org donor
  5. Neela Pal (D, Lapsed, Believer) — minor hallucination: is_oc_donor=false but GPT assigned lapsed (1/317 = 0.3% error)
- Verified lifecycle accuracy: 7/8 lapsed have is_oc_donor=true; 4 prior_donor with is_oc_donor=false but known_donor=true (valid signal)
- Cross-tab: All 25 List A contacts are Believers (correct per design)
**Learnings:**
- Campaign universe is 317, not ~200 as estimated in PRD — broader than expected
- Believer persona is over-assigned (139 vs expected 15-20) — GPT applies it liberally to anyone with OC engagement or comms history
- Network Peer is under-assigned (27 vs expected 80-120) — many Google/HBS contacts get Believer or Impact Pro instead
- This is acceptable for the campaign — better to over-personalize than under-personalize
- GPT-5 mini batch throughput: 3.8 contacts/sec with 150 workers (much faster than single-contact test suggested)
- Lifecycle stage detection has ~0.3% hallucination rate on donor status — acceptable
- The `known_donor` field provides a valid backup signal when `is_oc_donor` is absent
