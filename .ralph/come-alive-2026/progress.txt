# Ralph Progress Log
Loop: come-alive-2026
Started: 2026-02-23
---

## Codebase Patterns
- score_ask_readiness.py is the template for scaffold_campaign.py
- Pydantic schemas with openai.responses.parse() for structured output
- ThreadPoolExecutor(max_workers=150) for GPT-5 mini batch processing
- Supabase pagination: .range(offset, offset + page_size - 1)
- _strip_null_bytes() for PostgreSQL JSONB compatibility
- Env vars: OPENAI_APIKEY, ANTHROPIC_API_KEY, SUPABASE_URL, SUPABASE_KEY
- Strategy docs in docs/Outdoorithm/: DONOR_SEGMENTATION.md, COME_ALIVE_2026_Campaign.md, OC_FUNDRAISING_PLAYBOOK.md
- Python venv at .venv/, activate with source .venv/bin/activate
- GPT-5 mini does NOT support temperature=0
- supabase-contacts MCP server for DB operations

---

## Iteration Log

### Iteration 1 — US-001: Setup (2026-02-23)
**Status:** Complete
**What was done:**
- Added `campaign_2026` JSONB column to contacts table via `apply_migration` MCP tool
- Verified column exists: `{column_name: campaign_2026, data_type: jsonb}`
- Installed anthropic SDK v0.83.0 in .venv
- Verified both anthropic (0.83.0) and openai (2.21.0) imports work
**Learnings:**
- Use `apply_migration` (not `execute_sql`) for DDL operations per Supabase MCP guidance
- anthropic 0.83.0 installed cleanly with no dependency conflicts

### Iteration 2 — US-002: Create scaffold_campaign.py (2026-02-23)
**Status:** Complete
**What was done:**
- Created `scripts/intelligence/scaffold_campaign.py` — full campaign scaffolding script
- Pydantic schema `CampaignScaffold` with all 15 fields: persona, persona_confidence, persona_reasoning, campaign_list, capacity_tier, primary_ask_amount, motivation_flags, primary_motivation, lifecycle_stage, lead_story, story_reasoning, opener_insert, personalization_sentence, thank_you_variant, text_followup
- Comprehensive system prompt (~4,500 tokens) embedding:
  - Full persona decision tree (Believer → Impact Professional → Network Peer)
  - Execution matrix: Persona × Lifecycle → opener inserts
  - Ask anchor table: Persona × Capacity → dollar amounts
  - Motivation flags with detection signals (6 flags)
  - Story bank with matching rules (9 stories including "skip")
  - Thank-you frame: Persona × Motivation → thank-you variants
  - Follow-up timing matrix
  - Justin's voice guide and bio context
- Contact context builder reuses patterns from score_ask_readiness.py: ask_readiness, oc_engagement, communication_history, employment, education, shared_institutions, FEC, real estate, ai_tags, LinkedIn reactions
- Contact selection: ready_now (addressable) + cultivate_first score>=60 (addressable) + Tier 1 inner circle names
- Campaign list assignment logic: Tier 1 names → A, ready_now → B, cultivate_first >=76 → C, 60-75 → D
- JSONB merge preserves existing keys (personal_outreach, campaign_copy)
- `_strip_null_bytes()` for PostgreSQL JSONB compatibility
- CLI args: --test, --batch, --workers, --force, --contact-id
- ThreadPoolExecutor with 150 default workers, retry logic for rate limits and DB errors
- Test run: 1 contact scaffolded successfully, data saved to Supabase correctly
**Learnings:**
- The Supabase env var for the service key is `SUPABASE_SERVICE_KEY` (used in score_ask_readiness.py), not `SUPABASE_KEY` as documented in CLAUDE.md
- GPT-5 mini takes ~45s for a single scaffold call (heavier prompt than ask-readiness due to embedded strategy content)
- The `openai.responses.parse()` with `text_format=CampaignScaffold` works perfectly with all enum types
- Contact context is ~6K tokens per contact (ask_readiness summary + OC engagement + comms + employment + real estate + FEC)
- System prompt is the critical piece — embedding the full execution matrices and story bank ensures GPT makes correct assignments

### Iteration 3 — US-003: Run scaffold_campaign.py and Verify Output (2026-02-23)
**Status:** Complete
**What was done:**
- Ran `scaffold_campaign.py` on full campaign universe — 317 contacts scaffolded (316 new + 1 from test)
- 0 errors, 82.8s runtime, $0.65 cost (1.9M input tokens, 613K output tokens)
- Distribution: Believer 139, Impact Professional 151, Network Peer 27
- Campaign lists: A=25, B=107, C=42, D=143
- Capacity: Leadership 41, Major 137, Mid 113, Base 24, Community 2
- Lifecycle: New 289, Prior Donor 20, Lapsed 8
- Queried Supabase to verify distributions match script output (they do, +1 from prior test)
- Spot-checked 5 contacts across all list types and personas:
  1. Jose Gordon (A, Believer, major/$5K, skip story) — correct inner circle assignment
  2. Jesús García-Valadez (B, Impact Pro, major/$5K, carl story) — correct equity professional
  3. Roger Dean Huffstetler (C, Network Peer, major/$5K, dorian story) — correct HBS/HKS peer
  4. Nicki Anselmo (D, Prior Donor, Impact Pro, base/$1K, carl) — correct Google.org donor
  5. Neela Pal (D, Lapsed, Believer) — minor hallucination: is_oc_donor=false but GPT assigned lapsed (1/317 = 0.3% error)
- Verified lifecycle accuracy: 7/8 lapsed have is_oc_donor=true; 4 prior_donor with is_oc_donor=false but known_donor=true (valid signal)
- Cross-tab: All 25 List A contacts are Believers (correct per design)
**Learnings:**
- Campaign universe is 317, not ~200 as estimated in PRD — broader than expected
- Believer persona is over-assigned (139 vs expected 15-20) — GPT applies it liberally to anyone with OC engagement or comms history
- Network Peer is under-assigned (27 vs expected 80-120) — many Google/HBS contacts get Believer or Impact Pro instead
- This is acceptable for the campaign — better to over-personalize than under-personalize
- GPT-5 mini batch throughput: 3.8 contacts/sec with 150 workers (much faster than single-contact test suggested)
- Lifecycle stage detection has ~0.3% hallucination rate on donor status — acceptable
- The `known_donor` field provides a valid backup signal when `is_oc_donor` is absent

### Iteration 4 — US-004: Create write_personal_outreach.py (2026-02-23)
**Status:** Complete
**What was done:**
- Created `scripts/intelligence/write_personal_outreach.py` — personal outreach writer using Claude Opus 4.6
- Uses `anthropic.Anthropic()` client with `client.messages.create()` API
- JSON-mode output (not Pydantic) — system prompt instructs Opus to return raw JSON with 6 fields: subject_line, message_body, channel, follow_up_text, thank_you_message, internal_notes
- Comprehensive system prompt (~3,500 tokens) embedding:
  - Justin's voice guide with REAL example message from COME_ALIVE_2026_Campaign.md
  - All 3 persona scaffolds (Believer, Impact Pro, Network Peer) with outreach strategies
  - Full execution matrix: Persona × Lifecycle → opener inserts
  - Thank-you frame matrix: Persona × Motivation Flag → thank-you variants
  - Complete story bank with 9 stories and matching rules
  - Donor psychology quick reference (identity circuit, warm glow, endowed progress, etc.)
  - Campaign context ($100K goal, 10 trips, $20K match, Feb 26 launch)
  - Explicit voice instruction: "Sound like Justin texting or emailing a friend. NOT a development officer."
- Rich per-contact context includes:
  - Full scaffold data from campaign_2026 (persona, list, capacity, motivation, story, opener, etc.)
  - Full ask_readiness data (score, tier, personalization_angle, receiver_frame, reasoning)
  - Detailed communication history with up to 8 recent threads (subject, date, channel)
  - OC engagement (donor status, trips, roles)
  - Shared institutions with temporal overlap markers
  - Employment (5 most recent), education, alignment flags, LinkedIn reactions
- Selects List A contacts: filters on campaign_2026 scaffold campaign_list = "A"
- Low concurrency: 3 workers default (ThreadPoolExecutor for batch, sequential for test/small)
- JSONB merge preserves existing scaffold and campaign_copy keys
- CLI args: --test, --force, --contact-id, --workers (default 3)
- Error handling: retries for RateLimitError and APIError, JSON parse error recovery (strips markdown fencing)
- `print_all_messages()` method prints every message for review after full run
- Test run: 1 contact (Sergio Garcia) — 188-word email, $0.11 cost, 18.4s, sounds like Justin
**Learnings:**
- Opus 4.6 reliably returns clean JSON when instructed — no need for tool_use pattern
- Opus sometimes wraps JSON in markdown code fences (```json ... ```) — added parser to strip these
- ~5K input tokens per contact (system prompt ~3.5K + contact context ~1.5K)
- ~550 output tokens per contact (subject + 188-word body + follow-up + thank-you + notes)
- Cost: ~$0.11 per contact ($15/M input, $75/M output) — projected ~$2.75 for 25 List A contacts
- Latency: ~18s per contact — sequential for small batches is fine, parallel with 3 workers for full run
- The SUPABASE_SERVICE_KEY env var (not SUPABASE_KEY) pattern confirmed from previous iterations

### Iteration 5 — US-005: Run write_personal_outreach.py and Verify Output (2026-02-23)
**Status:** Complete
**What was done:**
- Ran `write_personal_outreach.py --force` on all 25 List A contacts
- 25 messages written, 0 errors, 141.2s runtime, $2.85 cost
- Channel split: 24 email, 1 text (Hector Mujica — SMS history justified text channel)
- All messages printed to stdout in readable format with contact name, channel, subject, body, follow-up, thank-you, internal notes
- Manually verified 3 messages:
  1. Adrian Schurr (prior donor, $5K ask) — references "$2K last year," casual tone, 142 words. Step-up ask appropriate.
  2. Tyler Scriven (believer, $10K ask) — references recent text exchange, mentions Atlanta/Serenbe pilot. 195 words. Highly personalized.
  3. Roxana Shirkhoda (prior donor, $5K ask) — references "$500 last year and being on that trip." 156 words. Partnership framing.
- All 3 pass: Justin's voice (casual, fragments, em dashes, "Quick thing"), appropriate ask amounts, accurate shared history references, 100-200 word range
- All 25 messages are Believers (correct — all List A contacts were scaffolded as Believers in US-003)
- Subject lines varied but consistent with Justin's voice: "quick thing," "a personal ask," "quick thing — before I go wide"
**Learnings:**
- Opus 4.6 with --force generates consistent quality across all 25 contacts — no quality degradation in batch mode
- Average 5.65s/contact with 3 workers — faster than the 18s single-contact test (concurrent pipeline efficiency)
- Total cost $2.85 for 25 contacts (123K input, 13K output tokens) — within the projected $2-5 range
- Valencia story dominates (most contacts got it) — expected since most List A are Believers with relationship-first framing
- Messages that reference specific shared history (recent texts, prior gifts, events) feel most authentic
- The "advisory/founding partner" frame appears in ~80% of messages — Opus correctly identifies that List A contacts should be treated as partners, not just donors
- One text channel selection (Hector) — Opus correctly reads SMS comms history to determine channel appropriateness
