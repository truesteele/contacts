# Ralph Progress Log
Loop: network-intel-overhaul
Started: 2026-02-20
---

## Iteration 1: US-001 — Update Network Intelligence System Documentation
**Date:** 2026-02-20
**Status:** COMPLETE
**Commit:** 774a87a

**What was done:**
- Expanded Phase 6 section (14) of `docs/NETWORK_INTELLIGENCE_SYSTEM.md` with full donor psychology framework
- Added detailed documentation for all 4 pillars: Relationship Depth, Giving Capacity, Philanthropic Propensity, Psychological Readiness
- Added Critical Behavioral Insights section (insider effect, identifiable victim effect, social proof, loss aversion, second-gift psychology, cultivation timelines)
- Added per-contact context template and output schema for ask-readiness scoring
- Added detailed scoring tier descriptions with guidance
- All JSONB schemas already documented (fec_donations, real_estate_data, shared_institutions, ask_readiness)
- Implementation order and cost estimates already present

**Learnings:**
- The doc already had a comprehensive Phase 6 section (14.1-14.10) from prior planning work. The main gap was that section 14.7 (Ask-Readiness) only had brief bullet points for the 4 pillars — needed expansion with full framework details and behavioral insights from the plan file.
- The JSONB schemas, search system overhaul, UI updates, and implementation status were already well-documented.

## Iteration 2: US-002 — Database Migration — New Columns & Indexes
**Date:** 2026-02-20
**Status:** COMPLETE

**What was done:**
- Created migration file `supabase/migrations/20260220_network_intel_overhaul.sql`
- Added 5 new columns: `comms_last_date` (DATE), `comms_thread_count` (SMALLINT default 0), `fec_donations` (JSONB), `real_estate_data` (JSONB), `ask_readiness` (JSONB)
- Renamed existing `shared_institutions` TEXT[] column to `shared_institutions_legacy` (contained freetext like "Location: SF", "Current employer: Google")
- Created new `shared_institutions` JSONB column for structured overlap data (to be populated by score_overlap.py in US-006)
- Created 3 indexes: `idx_contacts_familiarity` (btree DESC), `idx_contacts_comms_last` (btree DESC), `idx_contacts_ask_readiness` (GIN)
- Applied migration via Supabase MCP `apply_migration`
- Verified all columns and indexes exist via SQL queries
- `familiarity_rating` already existed from prior migration (20260219)

**Learnings:**
- The `shared_institutions` column already existed as TEXT[] with freetext data from the tagging script. Renamed to `shared_institutions_legacy` to preserve data while creating the new JSONB column. The legacy data is mostly location strings and employer names — the new structured format will have temporal overlap analysis.
- No TypeScript code referenced `shared_institutions` yet, so the rename was safe. The only Python reference is in `deduplicate_contacts.py` (a completed one-time script).
- `familiarity_rating` column already existed from migration 20260219120000, so the plan's `ADD COLUMN IF NOT EXISTS` for that was unnecessary — just added the index.

## Iteration 3: US-003 — Backfill Comms Summary Fields
**Date:** 2026-02-20
**Status:** COMPLETE

**What was done:**
- Created `scripts/intelligence/backfill_comms_fields.py` — simple script to denormalize `communication_history` JSONB into `comms_last_date` and `comms_thread_count` columns
- Reads `communication_history->>'last_contact'` → `comms_last_date` (DATE) and `communication_history->>'total_threads'` → `comms_thread_count` (SMALLINT)
- Uses Supabase pagination with `.range()` pattern for fetching contacts
- Supports `--test` flag (preview 5 contacts without writing)
- Full run: 628 contacts updated, 0 errors, 34.4s
- Verified: `SELECT count(*) FROM contacts WHERE comms_last_date IS NOT NULL` = 628

**Learnings:**
- The `communication_history` JSONB stores `total_threads` and `last_contact` as strings (JSON text), so parsing `int(total_threads)` is needed for the SMALLINT column.
- The `comms_last_date` DATE column accepts the "YYYY-MM-DD" string directly from the JSONB without conversion.
- Data spans from 2007-07-16 to 2026-02-19, with avg 2.7 threads per contact and max 50.

## Iteration 4: US-004 — FEC Political Donation Enrichment Script
**Date:** 2026-02-21
**Status:** COMPLETE
**Commit:** 242b07d

**What was done:**
- Created `scripts/intelligence/enrich_fec_donations.py` — queries OpenFEC `/schedules/schedule_a/` for each contact
- Filters by cycles 2020-2026, paginates through all results
- Disambiguates results with name matching (handles compound last names like "Kapor Klein" → FEC stores as "KLEIN" with "KAPOR" in middle name)
- For compound last names, verifies all name parts appear in `contributor_name` field
- State-based disambiguation for common names (>10 results → prefer state matches)
- Stores JSONB with: total_amount, donation_count, max_single, cycles, recent_donations (top 10), employer_from_fec, occupation_from_fec, last_checked
- Rate limited to 3.7s between calls (~970/hr, safely under 1,000/hr limit)
- Sequential processing (parallelism not useful due to rate limit)
- Supports --test (1 contact), --batch N, --start-from flags
- Tested: batch of 3 contacts all found FEC records (e.g., Freada Kapor Klein: $233K across 83 donations)

**Learnings:**
- OpenFEC API uses `contributor_name` field for full-text search in "LAST, FIRST MIDDLE" format. The search is fuzzy/keyword — searching "KAPOR KLEIN, FREADA" returns results where name is "KLEIN, FREADA KAPOR".
- `contributor_first_name` and `contributor_last_name` are decomposed fields but don't handle compound names well — "KLEIN" is the last name while "KAPOR" ends up in the middle/first name area.
- The API supports `is_individual=true` to filter out PAC-to-PAC transfers.
- Pagination uses `last_index` + `last_contribution_receipt_date` cursor-based approach, not offset-based.
- Results include employer and occupation from FEC filings, which can cross-validate LinkedIn data.
- Rate limit: 1,000 requests/hour with API key. At 3.7s delay, full 2,400 contacts will take ~2.5 hours.

## Iteration 5: US-005 — Real Estate Holdings Enrichment Script
**Date:** 2026-02-21
**Status:** COMPLETE

**What was done:**
- Created `scripts/intelligence/enrich_real_estate.py` — production three-step pipeline script
- Step 1: Batch skip-trace via Apify `one-api/skip-trace` (25 names per batch, $0.007/result)
- Step 2: Zillow autocomplete API for ZPID (free, per-address sequential)
- Step 3: Batch Zillow detail via Apify `happitap/zillow-detail-scraper` (25 URLs per batch, ~$0.003/result)
- GPT-5 mini validates each skip-trace result — correctly rejects wrong-person matches
- Only processes contacts with `familiarity_rating >= 2 OR ai_capacity_tier = 'major_donor'`
- Stores full JSONB: address, zestimate, rent_zestimate, beds, baths, sqft, year_built, property_type, confidence, source, last_checked
- Three confidence/source states: "zillow_via_skip_trace" (full success), "skip_trace_only" (address but no ZPID), "skip_trace_rejected" (wrong person), "skip_trace_failed" (no result)
- Supports --test (1 contact), --batch N, --start-from flags
- Tested: batch of 5 contacts → 4 addresses found, 4/4 validated, 4/4 ZPIDs, 4/4 Zestimates (80% end-to-end)
- Results: Zestimates ranged $462K-$1.1M across test contacts

**Learnings:**
- The skip-trace API returns results keyed by "Input Given" field matching the input name string. Must try multiple key formats (name + city + state, name + state, name alone) to match.
- GPT-5 mini validation works well — the first test contact (Kay Fernandez Smith) was correctly rejected because the skip-trace found someone in Travelers Rest, SC when contact is in a different location.
- Batch skip-trace is efficient — 5 contacts in a single Apify run takes ~8s. The Zillow detail batch also handles multiple URLs in one run.
- Zillow autocomplete is very reliable — 100% ZPID found for all validated addresses in testing.
- The `$ ` prefix in the Zillow detail print was a minor bug (should be contact name not dollar sign) but doesn't affect functionality.
- Rejected and no-result contacts get stored with appropriate markers so they won't be re-processed on subsequent runs.

