# LinkedIn News Scraper - Daily Digest
# Runs every day at 7am Pacific (14:00 UTC in winter, 15:00 UTC in summer)

name: Daily News Digest

on:
  # Run daily at 7am Pacific
  schedule:
    - cron: '0 15 * * *'  # 15:00 UTC = 7am PST / 8am PDT

  # Run tests on push to news_scraper files
  push:
    paths:
      - 'scripts/news_scraper/**'
      - '.github/workflows/news-scraper.yml'

  # Run tests on PR
  pull_request:
    paths:
      - 'scripts/news_scraper/**'

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Run without sending email'
        required: false
        default: 'false'
        type: boolean
      skip_tests:
        description: 'Skip smoke tests'
        required: false
        default: 'false'
        type: boolean

jobs:
  # Run smoke tests first (no API keys required)
  test:
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_tests != 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'scripts/news_scraper/requirements.txt'

      - name: Install dependencies
        run: |
          cd scripts/news_scraper
          pip install -r requirements.txt

      - name: Run smoke tests
        run: |
          cd scripts/news_scraper
          pytest test_smoke.py -v --tb=short

  # Run the actual scraper (only on schedule or manual trigger)
  scrape-and-send:
    runs-on: ubuntu-latest
    needs: test
    if: |
      always() &&
      (needs.test.result == 'success' || needs.test.result == 'skipped') &&
      (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'scripts/news_scraper/requirements.txt'

      - name: Install dependencies
        run: |
          cd scripts/news_scraper
          pip install -r requirements.txt

      - name: Validate secrets are configured
        run: |
          if [ -z "${{ secrets.OPENAI_API_KEY }}" ]; then
            echo "::error::OPENAI_API_KEY secret is not configured"
            exit 1
          fi
          if [ -z "${{ secrets.SENDGRID_API_KEY }}" ]; then
            echo "::error::SENDGRID_API_KEY secret is not configured"
            exit 1
          fi
          if [ -z "${{ secrets.RECIPIENT_EMAIL }}" ]; then
            echo "::error::RECIPIENT_EMAIL secret is not configured"
            exit 1
          fi
          if [ -z "${{ secrets.SENDER_EMAIL }}" ]; then
            echo "::error::SENDER_EMAIL secret is not configured"
            exit 1
          fi
          echo "All required secrets are configured"

      - name: Run news scraper
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENT_EMAIL }}
          SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
          MAX_STORIES: '10'
          HOURS_LOOKBACK: '24'
        run: |
          cd scripts/news_scraper
          if [ "${{ github.event.inputs.dry_run }}" = "true" ]; then
            python main.py --dry-run
          else
            python main.py
          fi

      - name: Upload digest artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: news-digest-${{ github.run_number }}
          path: scripts/news_scraper/output/
          retention-days: 30

      - name: Report failure
        if: failure()
        run: |
          echo "::error::News scraper failed to complete"
          echo "Check the logs above for details"
